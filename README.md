<div align="center">
    <img src="statics/logo.png" width="250" />
    <h4>
        An open-source CLI app to build your entire ML project, from research to production. <br />
        Piecutter-CLI 0.2.0 is released! :rocket:
    </h4>
    <p>
        <img src="https://img.shields.io/pypi/pyversions/piecutter-cli" />
        <img alt="PyPI" src="https://img.shields.io/pypi/v/piecutter-cli">
        <img src="https://img.shields.io/pypi/l/piecutter-cli" />
    </p>
    <p>:recycle: As soon as possible I'll be rewriting this entire package. For now, it's unavailable.</p>
</div>


## :notebook: Base Piecutter Project Structure
The project structure generated by Piecutter for the research/modeling phase of your project.

    ------------
       ├── README.md             -> The README.md file for describing your project.
       ├── requirements.txt      -> List of requirements to run your code.
       ├── service.py            -> Production service file to run BentoML models.
       ├── bentofile.yaml        -> YAML file for BentoML build (Do not change this file name!).
       ├── api_config.yaml       -> YAML file for BentoML's API configuration.
       ├── data                  -> The dataset of your project at different stages.
       │        ├── raw
       │        ├── processed
       │        ├── finalized
       ├── notebooks             -> Your jupyter notebooks.
       ├── references            -> Any external reference used in your project.
       ├── reports               -> Reports folder to store figures and tables.
       │        ├── figures",
       │        ├── tables",
       ├── scripts               -> Python scripts for training/saving models.
       ├── models                -> Serialized models will be stored here!
    ------------

## :interrobang: Why Piecutter?
Piecutter CLI is a project highly inspired by the well-known <a href="https://github.com/cookiecutter/cookiecutter" target="_blank">Cookiecutter</a> project.

*But why another CLI app inspired in a well established one?*

A lot of data scientists need to put models into production right after the modeling phase :crystal_ball:, and Cookiecutter doesn't help with this important step of the machine learning lifecycle :recycle:. Moreover, the project template generated by Cookiecutter :cookie: has some files and folders that we, as data scientists, don't use very often in a research-to-production environment :microscope:. Piecutter generates a much more cleaner structure of folders and files for the research phase of a ML project.

*And about the production phase?*

Piecutter implements a standardized way to package :zap: and put trained models and ML pipelines into production by using BentoML :rocket:. BentoML is a tool to standardize the process of ML model deployment by building an inference API around your trained pipeline as well as containerizing this application with docker :whale:, making it available for deployment right off the bat.

*But why should I use Piecutter CLI if BentoML exists?*

Well, BentoML's development is in full swing :steam_locomotive:, but this is good and bad at the same time. Although very standardized, the BentoML team constantly make significant changes on the package design :fire:, which affects not only the user experience, but also makes the official documentation outdated very fast :cyclone:. Piecutter puts all this mess :poop: out of your sight and gives you few commands for you to generate your entire research environment structure as well as to generate your production-ready inference API code in a matter of seconds :alarm_clock:!

Moreover, BentoML isn't capable of generating the research and the production structure in the same codebase :computer:, it isn't meant for that actually. Piecutter :cake: takes care of this integration and on top of that implements *Custom Runnables* for any unsupported framework as well as for more complex AI pipelines just by running one or two commands :tada:.

## Features
+ Cross-platform: Windows, Linux and Mac are supported.
+ Works with Python 3.8, or a newer version.
+ Code formatting with Python Black.

## :alien: Who Should Use Piecutter?
+ Someone who wants a standard and concise data science project template.
+ Anyone starting a career in data science.
+ People who like low-code solutions in Python.
+ Data scientists with no experience in software engineering and want to go quickly from model research to package and production.

## :heavy_check_mark: Supported Frameworks
Piecutter automatically generates the production code for all BentoML officially supported frameworks!

## :bookmark_tabs: API Docs

### :page_with_curl: Table of Contents
1. :satellite: Installation
2. :heavy_check_mark: Check Version
3. :closed_book: Open Documentation
4. :new: Create a New Project
5. :carousel_horse: Add a New API Endpoint
6. :cloud: BentoML Deployment on AWS
***

###  1. Installation
Piecutter is available as a PyPI package, to install it, just run:

    $ pip install piecutter-cli

### 2. Check Version
Run the `version` command to check if piecutter is installed:

    $ piecutter version

### 3. Open Documentation
Use the `docs` command to quickly open the official documentation (this one you're reading) on your browser:

    $ piecutter docs

### 4. Create a New Project
To generate a new project template, run:

    $ piecutter new name_of_your_project --base-framework sklearn

The `piecutter new` command accepts two flags:
+ `--include-bento / --no-include-bento`: Which specifies if the buildable BentoML files will be written in the new project. You may want to `--no-include-bento` if your project isn't meant to generate a production-ready ML model/pipeline. *By default, Piecutter will always use `--include-bento` if it's not specified.*
    + You can always run `piecutter add bento-build --base-framework sklearn` inside your project root directory for Piecutter to create all the buildable BentoML files in case you not include it at project creation.

+ `--base-framework`: Which specifies for which supported ML/AI framework Piecutter will generates the production-ready code. Below there is a list of supported arguments.
    + catboost
    + fastai
    + keras
    + onnx
    + pytorch
    + pytorch_lightning
    + sklearn
    + tensorflow
    + transformers
    + xgboost
    + custom *(generates a custom Runnable for more complex pipelines)*

Right after creating a project, you may want to setup you environment and run the `pip install -r requirements.txt` command for installing BentoML dependencies.

### 5. Add a New API Endpoint
In order for testing BentoML during development as well as to send it to production, you need an API endpoint for inference via requests. Use the command below to add new API endpoints to your inference service, no matter which AI/ML framework you're using.

    $ piecutter add api-endpoint predict --input ARRAY --output JSON

The `piecutter add api-endpoint` command accepts two flags:
+ `--input`: Which specifies what type of input data the API should expects.
+ `--output`: Which specifies what type of output data the API should sends back.

Both of these flags expects the same set of possible arguments (you can use them interchangeably).
+ `ARRAY` for expecting/returning **Numpy Arrays**.
+ `DATAFRAME` for expecting/returning **Pandas Dataframes**.
+ `TEXT` for expecting/returning **Text**.
+ `IMAGE` for expecting/returning **Images**.
+ `JSON` for expecting/returning **JSON** (default).
+ `FILE` for expecting/returning **Files**.

### 6. BentoML Deployment on AWS
To deploy your BentoML service to AWS, please, head over to the <a href="https://docs.bentoml.org/en/0.13-lts/deployment/aws_ec2.html" target="blank">official BentoML AWS Deployment documentation</a>.
